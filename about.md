---
layout: page
title: Hi
permalink: /about/
---

I am a research engineer at FAIR, Meta, researching the use of language models in mathematics and theoretical physics. I graduated (a long time ago) from Ecole Polytechnique and ENSAE, where I majored in statistics. After a career in media, advertising, and software development, I had the good fortune of landing a short term contract as a visiting entrepreneur in Meta (best job title ever!), and publishing a [paper](https://arxiv.org/abs/1912.01412), with Guillaume Lample, demonstrating that transformers can be trained to perform symbolic integration, with the same accuracy as computer algebras. Then, one thing led to another, and I became, at the fresh age of 55, a full-time research scientist, working on AI4Science. My recent scientific news can usually be found on [my twitter account](https://twitter.com/f_charton). I can be contacted at [fcharton@gmail.com](mailto:fcharton@gmail.com)

### Publications
My full list of publications, and their citations can be found on [Google scholar](https://scholar.google.com/citations?hl=fr&user=1tMnd-4AAAAJ&view_op=list_works). Thanks to Kristin Lauter and Julia Kempe, I have an Erdos number of 3.

#### AI for maths
* [Deep learning for symbolic mathematics (2019)](https://arxiv.org/abs/1912.01412), with Guillaume Lample: transformers can learn to integrate functions, and solve first and second order ordinary differential equations ([code](https://github.com/facebookresearch/SymbolicMathematics)).
* [Learning advanced mathematical computations from examples (2020)](https://arxiv.org/abs/2006.06462), with Amaury Hayat and Guillaume Lample: learning proposerties of differential systems, convergence at a critical point (aka the Spectral Mapping Theorem), controllability of overparametrized systems, integrability of some partial differential equations ([code](https://github.com/facebookresearch/MathsFromExamples)). 
* [A deep language model to predict metabolic network equilibria (2021)](https://arxiv.org/abs/2112.03588), with Amaury Hayat, Sean McQuade, Nathaniel Merrill and Benedetto Piccoli: predicting properties of transport graphs, existence of an equilibrium, and flows at the equilibrium.
* [Linear algebra with transformers (2021)](https://arxiv.org/abs/2112.01898): learning basic operations on matrices (transposition, addition, multiplication), eigenvalue and singular value decomposition and matrix inversion. First results about out-of-distribution generalization: models can generalize if their training distribution is chosen wisely.
* [Deep Symbolic Regression for Recurrent Sequences (2021)](https://arxiv.org/abs/2201.04600), with Stéphane d'Ascoli, Pierre-Alexandre Kamienny and Guillaume Lample: recovering underlying recurrence relations from a sequence of numbers. When predicting the next terms in a sequence (e.g. IQ tests), discovering the law (symbolic regression) and then using it to predict outperforms direct prediction. 
* [End-to-end symbolic regression with transformers (2022)](https://arxiv.org/abs/2204.10532), with Pierre-Alexandre Kamienny, Stéphane d'Ascoli and Guillaume Lample: transformers can predict functions from their values, first attempt at a model that uses both numeric and symbolic tokens.

#### Maths for understanding AI
* [What is my math transformer doing? Three results on interpretability and generalization (2022)](https://arxiv.org/abs/2211.00170)
* [Length generalization in arithmetic transformers (2023)](https://arxiv.org/abs/2306.15400), with Samy Jelassi, Stéphane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, and Yuanzhi Li
* [Learning the greatest common divisor: explaining transformer predictions (2024)](https://arxiv.org/abs/2308.15594)

#### AI for Physics

#### Cryptanalysis
* [SALSA: attacking lattice cryptography with transformers (2022)](https://arxiv.org/abs/2207.04785), with Emily Wenger, Mingjie Chen, and Kristin Lauter 
* [SALSA PICANTE: a machine learning attack on LWE with binary secrets (2023)](https://arxiv.org/abs/2303.04178), with Cathy Li, Jana Sotakova, Mohamed Mahlou, Evrard Garcelon, Emily Wenger and Kristin Lauter
* [SALSA VERDE: a machine learning attack on Learning With Errors with sparse small secrets (2023)](https://arxiv.org/abs/2306.11641), with Cathy Li, Emily Wenger, Zeyuan Allen-Zhu, and Kristin Lauter

### Workshops I co-organized

* [Mathematics and Machine Learning program](https://cmsa.fas.harvard.edu/event/mml2024/), Harvard CMSA (Autumn 2024), with Michael Douglas (Harvard), Michael Freedman (Harvard), Geordie Williamson (Sydney Mathematicam Research Institute), Fabian Ruehle (Northwestern)
* [Maths for and by large language models](https://www.youtube.com/playlist?list=PLx5f8IelFRgHrJ9W6_fbfO3ahDrXMEIWn), IHES, May 2024, with Michael Douglas (Harvard) and Yiannis Vlassopoulos (IHES)


### Invited talks

