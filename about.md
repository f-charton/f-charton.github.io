---
layout: page
title: short bio
permalink: /about/
---

I am a Research Engineer at FAIR, Meta, researching the use of language models in mathematics and theoretical physics. I am also affiliated with Ecole des Ponts / CERMICS. I graduated from Ecole Polytechnique in 1987 and ENSAE (Ecole Nationale de la Statistique et de l'Administration Economique) in 1990, majoring in statistics. After a career in media, advertising and software development ([my LinkedIn profile](https://www.linkedin.com/in/fran%C3%A7ois-charton-214187120/), I joined Meta as a Research Engineer, working on AI for mathematics and theoretical physics ([Google Scholar](https://scholar.google.com/citations?hl=fr&user=1tMnd-4AAAAJ&pagesize=80&view_op=list_works)). 

I can be reached at [fcharton@gmail.com](mailto:fcharton@gmail.com). My recent scientific news can usually be found on [my twitter account](https://twitter.com/f_charton).



### Selected Publications
Full list on [Google scholar](https://scholar.google.com/citations?hl=fr&user=1tMnd-4AAAAJ&view_op=list_works). 

#### AI for Maths
* [Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers](https://openreview.net/pdf?id=kOMrm4ZJ3m), with Alberto Alfarano and Amaury Hayat, NeurIPS 2024
* [PatternBoost: Constructions in mathematics with a little help from AI](https://arxiv.org/abs/2411.00566), with Adam Wagner, Jordan Ellenberg and Geordie Williamson, generative models for combinatorics, 2024
* [Linear algebra with transformers (2021)](https://arxiv.org/abs/2112.01898), solo paper, transformers can learn linear algebra, matrix operations, eigen decomposition, TMLR 2022
* [Learning advanced mathematical computations from examples (2020)](https://arxiv.org/abs/2006.06462), with Amaury Hayat and Guillaume Lample: learning proposerties of differential systems, convergence at a critical point (aka the Spectral Mapping Theorem), controllability of overparametrized systems, integrability of some partial differential equations ([code](https://github.com/facebookresearch/MathsFromExamples)), ICLR 2021. 
* [Deep learning for symbolic mathematics (2019)](https://arxiv.org/abs/1912.01412), with Guillaume Lample: symbolic integration with trasnformers ([code](https://github.com/facebookresearch/SymbolicMathematics)), ICLR 2020.

#### AI for physics
* [Transforming the bootstrap: Using transformers to compute scattering amplitudes in planar n= 4 super yang-mills theory](https://arxiv.org/abs/2405.06107), with Tianji Cai, Garrett Mertz, Niklas Nolte, Kyle Cranmer, Matthias Wilhelm and Lance Dixon, MLST 2024
  

#### Maths for understanding AI
* [Learning the greatest common divisor: explaining transformer predictions (2024)](https://arxiv.org/abs/2308.15594), solo paper, ICLR 2024 spotlight
* [Emergent properties with repeated examples](https://arxiv.org/abs/2410.07041), with Julia Kempe, won the Debunking Challenge, in the NeurIPS 2024 workshop on scientific methods for understanding deep learning
* [A tale of tails: Model collapse as a change of scaling laws](), with Elvis Dohmatob, Yunzhen Feng, Pu Yang and Julia Kempe, ICML 2024 
* [Length generalization in arithmetic transformers](https://arxiv.org/abs/2306.15400), with Samy Jelassi, Stéphane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, and Yuanzhi Li, 2023
* [What is my math transformer doing? Three results on interpretability and generalization (2022)](https://arxiv.org/abs/2211.00170)

#### Symbolic regression
* [End-to-end symbolic regression with transformers (2022)](https://arxiv.org/abs/2204.10532), with Pierre-Alexandre Kamienny, Stéphane d'Ascoli and Guillaume Lample: trasnformer based symbolic regression, NeurIPS 2022.
* [Deep Symbolic Regression for Recurrent Sequences (2021)](https://arxiv.org/abs/2201.04600), with Stéphane d'Ascoli, Pierre-Alexandre Kamienny and Guillaume Lample: recovering underlying recurrence relations from a sequence of numbers, ICML 2022. 

#### Cryptanalysis
* [SALSA: attacking lattice cryptography with transformers (2022)](https://arxiv.org/abs/2207.04785), with Emily Wenger, Mingjie Chen, and Kristin Lauter, NeurIPS 2022
* [SALSA PICANTE: a machine learning attack on LWE with binary secrets (2023)](https://arxiv.org/abs/2303.04178), with Cathy Li, Jana Sotakova, Mohamed Mahlou, Evrard Garcelon, Emily Wenger and Kristin Lauter, CCS 2023
* [SALSA VERDE: a machine learning attack on Learning With Errors with sparse small secrets (2023)](https://arxiv.org/abs/2306.11641), with Cathy Li, Emily Wenger, Zeyuan Allen-Zhu, and Kristin Lauter, NeurIPS 2023

### Workshops and Programs I co-organized

* [Mathematics and Machine Learning program](https://cmsa.fas.harvard.edu/event/mml2024/), Harvard CMSA (Autumn 2024), with Michael Douglas (Harvard), Michael Freedman (Harvard), Geordie Williamson (Sydney Mathematicam Research Institute), Fabian Ruehle (Northwestern)
* [Maths for and by large language models](https://www.youtube.com/playlist?list=PLx5f8IelFRgHrJ9W6_fbfO3ahDrXMEIWn), IHES, May 2024, with Michael Douglas (Harvard) and Yiannis Vlassopoulos (IHES)


### Invited talks

